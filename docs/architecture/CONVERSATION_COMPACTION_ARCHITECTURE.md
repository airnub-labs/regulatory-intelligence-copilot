# Conversation Compaction Architecture

> **Version**: 1.0
> **Status**: ğŸ”µ Planned (Implementation Ready)
> **Created**: 2025-12-30
> **Related Documents**:
> - `MESSAGE_PINNING.md` (Phase 3 - Compaction Strategies)
> - `conversation-branching-and-merging.md` (Merge Operations)
> - `conversationConfig.ts` (Configuration System)
> - Implementation Plan: `docs/development/implementation-plans/COMPACTION_STRATEGIES_IMPLEMENTATION_PLAN.md`

---

## Executive Summary

This document describes the **Conversation Compaction System** - a comprehensive architecture for managing conversation context size through intelligent message compression and summarization strategies.

### The Problem

**Context Bloat in Conversational AI Systems**:

When users engage in long conversations or merge large branches, the conversation context can grow unbounded, leading to:

- **Token limit exceeded errors** - LLM calls fail when context exceeds model limits (e.g., 128k tokens)
- **Degraded response quality** - Irrelevant historical context dilutes the signal
- **Increased API costs** - Larger contexts consume more tokens per request
- **Poor user experience** - Slow responses, failed merges, confusing errors

**Current Gap**:

The Regulatory Intelligence Copilot has:
- âœ… Configuration framework for compaction strategies (`conversationConfig.ts`)
- âœ… Message pinning to mark important content
- âœ… AI-powered merge summarization (for `summary` merge mode)
- âŒ **No implementation of documented compaction algorithms**
- âŒ **No token counting infrastructure**
- âŒ **Full merge copies ALL messages verbatim** (no compression)
- âŒ **Paths can grow unbounded** (no automatic compaction)

### The Solution

**A Unified Compaction System** with:

1. **Token Counting Infrastructure** - Accurate measurement of context size using tiktoken
2. **8 Compaction Strategies** - Pluggable algorithms for different use cases
3. **Path Compaction** - Automatic compression of active conversation paths
4. **Merge Compaction** - Intelligent compression during branch merges
5. **Pinned Message Preservation** - Always preserve user-marked important content
6. **Configuration System** - Hierarchical (global/tenant/user) strategy selection

### Key Capabilities

| Capability | Description | Status |
|------------|-------------|--------|
| **Token Counting** | Measure conversation size in tokens (not just characters) | ğŸ”µ Planned |
| **Path Auto-Compaction** | Compress active paths when exceeding thresholds | ğŸ”µ Planned |
| **Merge Compaction** | Compress messages during branch merge operations | ğŸ”µ Planned |
| **Pinned Preservation** | Never remove pinned messages during compaction | ğŸ”µ Planned |
| **Configurable Strategies** | Choose compression strategy per tenant/user | ğŸ”µ Planned |
| **LLM-Powered Semantic** | Use AI to identify important vs redundant content | ğŸ”µ Planned |

---

## Table of Contents

1. [Architecture Overview](#1-architecture-overview)
2. [System Components](#2-system-components)
3. [Compaction Strategies](#3-compaction-strategies)
4. [Token Counting Infrastructure](#4-token-counting-infrastructure)
5. [Path Compaction](#5-path-compaction)
6. [Merge Compaction](#6-merge-compaction)
7. [Configuration System](#7-configuration-system)
8. [Data Model](#8-data-model)
9. [Integration Points](#9-integration-points)
10. [Performance & Scalability](#10-performance--scalability)
11. [Security & Privacy](#11-security--privacy)
12. [Future Enhancements](#12-future-enhancements)

---

## 1. Architecture Overview

### 1.1 System Context

The Compaction System sits between the conversation storage layer and the LLM orchestration layer:

```mermaid
graph TB
    subgraph "User Layer"
        UI[Web UI]
        API[Next.js API Routes]
    end

    subgraph "Compaction System"
        TC[Token Counter]
        PC[Path Compactor]
        MC[Merge Compactor]
        CS[Compaction Strategies]
    end

    subgraph "Storage Layer"
        PS[Path Store<br/>Supabase]
        MS[Message Store<br/>Supabase]
    end

    subgraph "LLM Layer"
        LR[LLM Router]
        OA[OpenAI API]
    end

    UI --> API
    API --> PC
    API --> MC
    PC --> TC
    MC --> TC
    PC --> CS
    MC --> CS
    CS --> LR
    PC --> PS
    MC --> PS
    PS --> MS
    LR --> OA

    style CS fill:#e1f5fe
    style TC fill:#fff3e0
    style PC fill:#e8f5e9
    style MC fill:#fce4ec
```

### 1.2 Core Principles

**1. Separation of Concerns**
- Token counting is independent infrastructure (can be used anywhere)
- Path compaction is orthogonal to merge compaction
- Strategies are pluggable and composable

**2. Pinned Message Preservation**
- ALL strategies MUST respect pinned messages (when configured)
- Pinned messages are never removed, always preserved verbatim
- Clear contract: `preservePinned: boolean` configuration

**3. Progressive Enhancement**
- Simple strategies first (none, sliding_window, minimal)
- LLM-powered strategies later (semantic, moderate, aggressive)
- Graceful degradation (fallback to simpler strategies on error)

**4. Configuration Over Code**
- Default strategies work well for most users
- Tenants can override at organization level
- Users can customize their personal preferences

**5. Observability**
- All compaction operations are logged
- Metrics track token savings and reduction rates
- Errors never block user operations (fail gracefully)

### 1.3 Design Patterns

**Strategy Pattern** - Pluggable compaction algorithms:
```typescript
interface MessageCompactor {
  compact(context: CompactionContext): Promise<CompactionResult>;
}

class SlidingWindowCompactor implements MessageCompactor { ... }
class SemanticCompactor implements MessageCompactor { ... }
```

**Factory Pattern** - Create appropriate compactor for strategy:
```typescript
function createPathCompactor(strategy: PathCompressionStrategy): MessageCompactor {
  switch (strategy) {
    case 'sliding_window': return new SlidingWindowCompactor();
    case 'semantic': return new SemanticCompactor();
    // ...
  }
}
```

**Template Method** - Base class handles common logic:
```typescript
abstract class MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    // Common: partition pinned vs unpinned
    const { pinned, unpinned } = this.partitionByPinned(context);

    // Strategy-specific: compact unpinned
    const compacted = await this.compactUnpinned(unpinned, context);

    // Common: merge and validate
    const result = this.merge(pinned, compacted);
    this.validatePinnedPreservation(context, result);
    return result;
  }

  protected abstract compactUnpinned(
    messages: ConversationMessage[],
    context: CompactionContext
  ): Promise<ConversationMessage[]>;
}
```

---

## 2. System Components

### 2.1 Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Compaction System                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Token Counting Infrastructure                   â”‚    â”‚
â”‚  â”‚  Package: @acme/reg-intel-core/tokens                   â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  - TokenCounter (interface)                             â”‚    â”‚
â”‚  â”‚  - TiktokenCounter (implementation)                     â”‚    â”‚
â”‚  â”‚  - CharacterEstimator (fallback)                        â”‚    â”‚
â”‚  â”‚  - TokenCache (LRU cache)                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â–²                                      â”‚
â”‚                           â”‚ depends on                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Core Compaction Engine                          â”‚    â”‚
â”‚  â”‚  Package: @acme/reg-intel-conversations/compaction      â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  - MessageCompactor (abstract base)                     â”‚    â”‚
â”‚  â”‚  - CompactionContext (shared state)                     â”‚    â”‚
â”‚  â”‚  - CompactionResult (unified output)                    â”‚    â”‚
â”‚  â”‚  - Utilities (dedup, merge, validate)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚              â–²                              â–²                    â”‚
â”‚              â”‚                              â”‚                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚    â”‚  Path Compaction  â”‚        â”‚  Merge Compaction â”‚         â”‚
â”‚    â”‚   Strategies      â”‚        â”‚   Strategies      â”‚         â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚    â”‚ - NoneCompactor   â”‚        â”‚ - NoneCompactor   â”‚         â”‚
â”‚    â”‚ - SlidingWindow   â”‚        â”‚ - MinimalComp.    â”‚         â”‚
â”‚    â”‚ - Semantic        â”‚        â”‚ - ModerateComp.   â”‚         â”‚
â”‚    â”‚ - Hybrid          â”‚        â”‚ - AggressiveComp. â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Configuration System                            â”‚    â”‚
â”‚  â”‚  Package: @acme/reg-intel-conversations                 â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  - ConversationConfig (types)                           â”‚    â”‚
â”‚  â”‚  - ConversationConfigStore (interface)                  â”‚    â”‚
â”‚  â”‚  - SupabaseConversationConfigStore (impl)               â”‚    â”‚
â”‚  â”‚  - CachingConversationConfigStore (caching layer)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Component Responsibilities

| Component | Responsibility | Package |
|-----------|----------------|---------|
| **TokenCounter** | Measure message and context size in tokens | `reg-intel-core/tokens` |
| **MessageCompactor** | Abstract base for all compaction strategies | `reg-intel-conversations/compaction` |
| **PathCompactor** | Compact active conversation paths | `reg-intel-conversations/compaction/path` |
| **MergeCompactor** | Compact messages during branch merge | `reg-intel-conversations/compaction/merge` |
| **CompactionContext** | Shared state passed to compaction strategies | `reg-intel-conversations/compaction/types` |
| **CompactionResult** | Standardized output from compaction | `reg-intel-conversations/compaction/types` |
| **ConversationConfigStore** | Store and retrieve compaction configuration | `reg-intel-conversations` |

---

## 3. Compaction Strategies

### 3.1 Strategy Overview

The system provides **8 distinct compaction strategies** divided into two categories:

**Path Compression Strategies** (applied to active conversation paths):
1. `none` - No compression
2. `sliding_window` - Keep recent N messages
3. `semantic` - LLM-based importance scoring
4. `hybrid` - Sliding window + semantic for old messages

**Merge Compression Strategies** (applied during branch merge):
1. `none` - Full copy, no compression
2. `minimal` - Deduplication only
3. `moderate` - Summarize redundant exchanges
4. `aggressive` - Extract outcomes only

### 3.2 Strategy Selection Matrix

| Use Case | Path Strategy | Merge Strategy | Rationale |
|----------|---------------|----------------|-----------|
| **Short conversations (<50 msgs)** | `none` | `none` or `minimal` | No compression needed |
| **Standard usage (50-200 msgs)** | `sliding_window` | `moderate` | Balance context and clarity |
| **Long conversations (200+ msgs)** | `hybrid` | `moderate` or `aggressive` | Aggressive compression needed |
| **Compliance/audit scenarios** | `none` or `sliding_window` | `minimal` | Preserve all content |
| **Research/exploration** | `semantic` | `moderate` | Keep important insights |
| **Quick summaries** | `hybrid` | `aggressive` | Maximum compression |

### 3.3 Path Compression Strategies (Detail)

#### 3.3.1 None Strategy

**Purpose**: No compression, preserve all messages.

**Algorithm**:
```typescript
async compact(context: CompactionContext): Promise<CompactionResult> {
  // Simply return all messages unchanged
  return {
    messages: context.messages,
    tokensAfter: context.currentTokens,
    tokensBefore: context.currentTokens,
    messagesRemoved: 0,
    messagesSummarized: 0,
    pinnedPreserved: context.pinnedMessageIds.size,
    strategy: 'none',
    success: true,
  };
}
```

**When to use**:
- Short conversations
- Compliance scenarios requiring full audit trail
- Testing/debugging

**Characteristics**:
- Zero token reduction
- Zero message loss
- Constant time O(1)

#### 3.3.2 Sliding Window Strategy

**Purpose**: Keep recent N messages + all pinned messages.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Take last N unpinned messages (the "window")
3. Optionally summarize old unpinned messages (those outside window)
4. Combine: pinned + (optional summary) + recent unpinned
5. Validate pinned preservation
```

**Diagram**:
```
Original (100 messages):
[------ old (50) ------][------ recent (50) ------]
   ^pinned^  ^pinned^

After Sliding Window (windowSize=50):
[summary]  [------ recent (50) ------]
 ^pinned^    ^pinned^

Result: 52 messages (50 recent + 2 pinned + 1 summary)
```

**Configuration**:
- `windowSize`: Number of recent messages to keep (default: 50)
- `generateSummaries`: Whether to summarize old messages (default: true)

**When to use**:
- Standard conversations
- When recent context is most important
- Predictable, consistent behavior needed

**Characteristics**:
- Moderate token reduction (30-50%)
- Preserves recent context
- Linear time O(n)

#### 3.3.3 Semantic Strategy

**Purpose**: Use LLM to score message importance, keep highest-scoring messages.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Score each unpinned message for importance (0.0-1.0) using LLM
3. Keep messages with score >= threshold
4. Optionally apply token budget (keep messages until budget exhausted)
5. Combine: pinned + high-importance messages
6. Validate pinned preservation
```

**LLM Scoring Prompt**:
```
Score each message for importance (0.0 = low, 1.0 = high).

High importance (0.8-1.0):
- Regulatory requirements or compliance issues
- Key decisions or conclusions
- Unique factual information

Medium importance (0.4-0.7):
- Supporting details or explanations
- Examples or clarifications

Low importance (0.0-0.3):
- Greetings, small talk
- Redundant information
```

**Configuration**:
- `semanticThreshold`: Minimum importance score to keep (default: 0.5)
- `maxTokens`: Token budget limit (optional)

**When to use**:
- Long conversations with varying importance
- When quality matters more than recency
- Research or analysis scenarios

**Characteristics**:
- High token reduction (40-70%)
- Preserves most important content
- Higher compute cost (LLM calls)
- Non-deterministic (LLM-dependent)

#### 3.3.4 Hybrid Strategy

**Purpose**: Combine sliding window + semantic scoring for optimal results.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Split unpinned into: recent (last N) and old (rest)
3. Keep ALL recent messages (sliding window)
4. Apply semantic scoring to old messages only
5. Keep high-scoring old messages
6. Combine: pinned + high-scoring old + recent
7. Validate pinned preservation
```

**Diagram**:
```
Original (100 messages):
[------ old (50) ------][------ recent (50) ------]

Step 1 - Partition by recency:
  old: [------------ 50 messages ------------]
  recent: [------------ 50 messages ------------]

Step 2 - Semantic scoring on old:
  old (scored): [0.3][0.8][0.2][0.9][0.4]... (50 messages with scores)
  Keep if score >= 0.5: [0.8][0.9]... (15 messages retained)

Step 3 - Combine:
  [pinned] + [high-scoring old (15)] + [recent (50)]
  Result: 67 messages
```

**Configuration**:
- `windowSize`: Size of recent window (default: 50)
- `semanticThreshold`: Importance threshold for old messages (default: 0.5)

**When to use**:
- Long conversations requiring balance
- When both recency and importance matter
- Production default for most scenarios

**Characteristics**:
- Moderate-high token reduction (35-60%)
- Best of both worlds (recency + importance)
- Moderate compute cost
- Most versatile strategy

### 3.4 Merge Compression Strategies (Detail)

#### 3.4.1 None Strategy

**Purpose**: Copy all messages verbatim during merge.

**Algorithm**:
```typescript
async compact(context: CompactionContext): Promise<CompactionResult> {
  // No compression - copy all messages
  return {
    messages: context.messages,
    tokensAfter: context.currentTokens,
    tokensBefore: context.currentTokens,
    messagesRemoved: 0,
    messagesSummarized: 0,
    pinnedPreserved: context.pinnedMessageIds.size,
    strategy: 'merge:none',
    success: true,
  };
}
```

**When to use**:
- Small branches (<10 messages)
- Audit/compliance scenarios
- User explicitly requests full merge

**Characteristics**:
- Zero token reduction
- Complete history preserved
- Constant time O(1)

#### 3.4.2 Minimal Strategy (Deduplication)

**Purpose**: Remove duplicate messages, keep everything else.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Deduplicate unpinned messages by content hash
3. Pinned messages NEVER removed (even if duplicate)
4. Combine: pinned + deduplicated unpinned
5. Validate pinned preservation
```

**Deduplication Logic**:
```typescript
function hashMessage(msg: ConversationMessage): string {
  return `${msg.role}:${msg.content.trim().toLowerCase()}`;
}

// Messages with same hash are considered duplicates
// Keep first occurrence, remove subsequent ones
```

**When to use**:
- Branches with repeated questions/answers
- Minimal compression needed
- Fast processing required

**Characteristics**:
- Low token reduction (5-15%)
- Removes only duplicates
- Fast linear time O(n)

#### 3.4.3 Moderate Strategy (Default)

**Purpose**: Summarize redundant exchanges, keep key decisions.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Deduplicate unpinned messages
3. Use LLM to identify redundant exchanges
4. Summarize redundant content into single message
5. For pinned messages, add immediate context (1 before, 1 after)
6. Combine: pinned with context + important + summarized
7. Validate pinned preservation
```

**Redundancy Detection Prompt**:
```
Identify redundant messages that don't add new information.

Keep messages that:
- Introduce new concepts or facts
- Make decisions or conclusions
- Provide unique examples or data

Mark as redundant:
- Repetitive explanations
- Already-answered questions
- Clarifications of clear concepts
```

**When to use**:
- Medium-large branches (10-50 messages)
- Standard production use case
- Balance compression and preservation

**Characteristics**:
- Moderate token reduction (30-50%)
- Preserves key content
- Moderate compute cost
- **Recommended default**

#### 3.4.4 Aggressive Strategy

**Purpose**: Extract only outcomes and final decisions.

**Algorithm**:
```
1. Partition messages into pinned and unpinned
2. Use LLM to extract outcomes/conclusions from unpinned
3. Create summary message with extracted outcomes
4. Combine: pinned + outcomes summary
5. Validate pinned preservation
```

**Outcome Extraction Prompt**:
```
Extract ONLY:
1. Final decisions made
2. Key conclusions reached
3. Action items identified
4. Important facts discovered

Omit all discussion, questions, and exploratory dialogue.
Format as a concise list of outcomes.
```

**When to use**:
- Large branches (50+ messages)
- Quick summaries needed
- Maximum compression required

**Characteristics**:
- High token reduction (60-90%)
- Preserves only outcomes
- High compute cost
- Potential information loss

### 3.5 Strategy Composition

Strategies can be composed:

```typescript
// Example: Moderate merge compaction leverages path compaction primitives
class MergeModerateCompactor extends MessageCompactor {
  private deduplicator = new MergeMinimalCompactor();

  async compact(context: CompactionContext): Promise<CompactionResult> {
    // First deduplicate
    const dedupResult = await this.deduplicator.compact(context);

    // Then identify redundant exchanges
    const { redundant, important } = await this.identifyRedundant(
      dedupResult.messages
    );

    // Summarize redundant
    const summarized = await this.summarize(redundant);

    // Combine
    return this.combine(important, summarized);
  }
}
```

---

## 4. Token Counting Infrastructure

### 4.1 Why Token Counting Matters

**Problem**: Character count is not token count.

- "Hello world" = 12 characters, but **2 tokens** (for GPT models)
- LLMs charge by tokens, not characters
- Context limits are in tokens (e.g., 128k tokens for GPT-4)
- Need accurate measurement for budgeting and compaction

### 4.2 Token Counter Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        TokenCounter Interface           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + estimateTokens(text): TokenEstimate  â”‚
â”‚  + estimateMessageTokens(msg): Estimate â”‚
â”‚  + estimateContextTokens(msgs): Estimateâ”‚
â”‚  + clearCache()                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–²
                 â”‚ implements
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TiktokenCounter  â”‚  â”‚ CharacterEstim. â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Uses tiktoken  â”‚  â”‚ - Uses char/3.5 â”‚
â”‚ - Exact counts   â”‚  â”‚ - Approximation â”‚
â”‚ - LRU cache      â”‚  â”‚ - Fast fallback â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 Tiktoken Integration

**Tiktoken** is OpenAI's official tokenizer library.

**Usage**:
```typescript
import { encoding_for_model } from '@dqbd/tiktoken';

const encoder = encoding_for_model('gpt-4');
const tokens = encoder.encode("Hello, world!");
console.log(tokens.length); // 4 tokens
encoder.free(); // Release resources
```

**Model Support**:
- GPT-4 (all variants)
- GPT-3.5 Turbo
- Text-davinci-003
- Custom: can add mappings for other models

### 4.4 Character-Based Fallback

When tiktoken unavailable (e.g., unsupported model), fall back to estimation:

**Rules of Thumb**:
- English text: ~4 characters per token
- Code/technical: ~3 characters per token
- Mixed: ~3.5 characters per token (average)

**Implementation**:
```typescript
function estimateTokensFromCharacters(text: string): number {
  const chars = text.length;
  const CHARS_PER_TOKEN = 3.5;
  return Math.ceil(chars / CHARS_PER_TOKEN);
}
```

**Accuracy**: Within 15-20% of actual token count (good enough for estimation).

### 4.5 Token Caching

**Problem**: Counting tokens repeatedly for same content is wasteful.

**Solution**: LRU cache mapping content â†’ token count.

```typescript
class TiktokenCounter {
  private cache = new LRUCache<string, number>({
    max: 1000, // Cache up to 1000 entries
    ttl: 1000 * 60 * 60, // 1 hour TTL
  });

  async estimateTokens(text: string): Promise<TokenEstimate> {
    // Check cache first
    const cached = this.cache.get(text);
    if (cached !== undefined) {
      return { tokens: cached, method: 'cached', isExact: true };
    }

    // Count tokens
    const tokens = this.encoder.encode(text).length;

    // Cache for next time
    this.cache.set(text, tokens);

    return { tokens, method: 'tiktoken', isExact: true };
  }
}
```

**Cache Hit Rate**: Typically 60-80% for repeated content.

### 4.6 Token Counting API

**Interface**:
```typescript
interface TokenCounter {
  // Estimate tokens for raw text
  estimateTokens(text: string): Promise<TokenEstimate>;

  // Estimate tokens for a message (includes role overhead)
  estimateMessageTokens(message: ConversationMessage): Promise<TokenEstimate>;

  // Estimate total tokens for conversation context
  estimateContextTokens(messages: ConversationMessage[]): Promise<TokenEstimate>;

  // Clear cache
  clearCache(): void;
}

interface TokenEstimate {
  tokens: number;           // Estimated token count
  method: 'tiktoken' | 'character-estimate' | 'cached';
  isExact: boolean;         // True for tiktoken, false for estimate
}
```

**Usage Examples**:
```typescript
const counter = createTokenCounterForModel('gpt-4');

// Count single text
const estimate1 = await counter.estimateTokens("Hello, world!");
// { tokens: 4, method: 'tiktoken', isExact: true }

// Count message (includes role formatting)
const msg: ConversationMessage = {
  role: 'user',
  content: 'What are the PRSI obligations?',
  // ... other fields
};
const estimate2 = await counter.estimateMessageTokens(msg);
// { tokens: 12, method: 'tiktoken', isExact: true }
// (8 for content + 4 for role formatting)

// Count conversation context
const messages: ConversationMessage[] = [/* ... */];
const estimate3 = await counter.estimateContextTokens(messages);
// { tokens: 3547, method: 'tiktoken', isExact: true }
// (sum of all messages + conversation overhead)
```

---

## 5. Path Compaction

### 5.1 What is Path Compaction?

**Path Compaction** is the process of reducing the size of an **active conversation path** by removing or summarizing messages while preserving important content.

**When it runs**:
1. **Automatic** - When path exceeds configured thresholds
2. **Manual** - User clicks "Compact Path" button
3. **On-demand** - Before expensive LLM calls to reduce token usage

### 5.2 Auto-Compaction Triggers

**Trigger Conditions** (any of these can trigger compaction):

1. **Message Count Threshold**:
   - Path has more messages than `pathMaxMessages` (default: 200)
   - Example: 215 messages â†’ trigger compaction

2. **Token Budget Threshold**:
   - Path token count exceeds 80% of max model context
   - Example: 102k tokens (GPT-4 128k limit) â†’ trigger compaction

3. **Time-Based**:
   - Periodic compaction check (default: hourly)
   - Only compacts if thresholds exceeded

**Configuration**:
```typescript
interface ConversationConfig {
  autoCompactEnabled: boolean;           // Enable auto-compaction
  compactionIntervalMinutes: number;     // How often to check
  pathMaxMessages: number;               // Message count trigger
  pathMaxTokens: number;                 // Token count trigger
  pathCompressionStrategy: PathCompressionStrategy; // Which strategy to use
}
```

### 5.3 Path Compaction Flow

```mermaid
sequenceDiagram
    participant User
    participant API
    participant PathStore
    participant Compactor
    participant TokenCounter
    participant LLM

    User->>API: POST /api/chat
    API->>PathStore: shouldCompactPath(pathId)
    PathStore->>PathStore: Check thresholds
    alt Needs Compaction
        PathStore->>PathStore: compactPath(pathId)
        PathStore->>TokenCounter: estimateContextTokens(messages)
        TokenCounter-->>PathStore: currentTokens
        PathStore->>Compactor: compact(context)
        alt Semantic/Hybrid Strategy
            Compactor->>LLM: Score messages for importance
            LLM-->>Compactor: Importance scores
        end
        Compactor->>Compactor: Apply strategy logic
        Compactor-->>PathStore: CompactionResult
        PathStore->>PathStore: Replace messages (transactional)
        PathStore-->>API: Compaction complete
    end
    API->>API: Continue with chat request
    API-->>User: Response
```

### 5.4 Path Compaction API

**Store Interface**:
```typescript
interface ConversationPathStore {
  /**
   * Check if path needs compaction based on thresholds
   */
  shouldCompactPath(input: {
    tenantId: string;
    pathId: string;
  }): Promise<boolean>;

  /**
   * Apply compaction to a path
   */
  compactPath(input: {
    tenantId: string;
    pathId: string;
    strategy?: PathCompressionStrategy;  // Override config
    config?: Partial<CompactionConfig>;   // Additional config
  }): Promise<CompactionResult>;
}
```

**Implementation**:
```typescript
async compactPath(input: {
  tenantId: string;
  pathId: string;
  strategy?: PathCompressionStrategy;
  config?: Partial<CompactionConfig>;
}): Promise<CompactionResult> {
  // 1. Load configuration
  const pathConfig = await this.configStore.getConfig({
    tenantId: input.tenantId,
  });
  const strategy = input.strategy ?? pathConfig.pathCompressionStrategy;

  // 2. Load messages and pinned IDs
  const messages = await this.resolvePathMessages({
    tenantId: input.tenantId,
    pathId: input.pathId,
  });
  const pinnedMessageIds = new Set(
    messages.filter(m => m.metadata?.pinned).map(m => m.id)
  );

  // 3. Count tokens
  const tokenCounter = createTokenCounterForModel('gpt-4');
  const currentTokens = (
    await tokenCounter.estimateContextTokens(messages)
  ).tokens;

  // 4. Create compaction context
  const context: CompactionContext = {
    messages,
    pinnedMessageIds,
    tokenCounter,
    llmRouter: this.llmRouter,
    currentTokens,
    targetTokens: input.config?.maxTokens ?? pathConfig.pathMaxTokens,
    config: {
      preservePinned: pathConfig.mergePreservePinned ?? true,
      windowSize: pathConfig.pathSlidingWindowSize,
      semanticThreshold: pathConfig.pathCompressionThreshold,
      ...input.config,
    },
  };

  // 5. Run compaction
  const compactor = createPathCompactor(strategy, context.config);
  const result = await compactor.compact(context);

  // 6. Update path (transactional)
  if (result.success) {
    await this.replacePathMessages({
      tenantId: input.tenantId,
      pathId: input.pathId,
      messages: result.messages,
    });

    // 7. Log metrics
    this.logger?.info?.('[PathStore] Path compaction completed', {
      pathId: input.pathId,
      strategy: result.strategy,
      tokensBefore: result.tokensBefore,
      tokensAfter: result.tokensAfter,
      reduction: calculateReduction(result.tokensBefore, result.tokensAfter),
      messagesRemoved: result.messagesRemoved,
    });
  }

  return result;
}
```

### 5.5 Message Replacement (Atomic Operation)

**Challenge**: Replacing messages must be atomic to avoid data corruption.

**Solution**: Use PostgreSQL function for transactional replacement.

```sql
-- PostgreSQL function for atomic message replacement
CREATE OR REPLACE FUNCTION replace_path_messages(
  p_path_id uuid,
  p_tenant_id uuid,
  p_messages jsonb
)
RETURNS void AS $$
BEGIN
  -- Start transaction (implicit in function)

  -- Soft delete existing messages
  UPDATE copilot_internal.conversation_messages
  SET deleted_at = now()
  WHERE path_id = p_path_id
    AND tenant_id = p_tenant_id
    AND deleted_at IS NULL;

  -- Insert compacted messages
  INSERT INTO copilot_internal.conversation_messages (
    id, tenant_id, conversation_id, path_id,
    role, content, sequence_in_path,
    is_branch_point, branched_to_paths,
    metadata, created_at
  )
  SELECT
    (msg->>'id')::uuid,
    p_tenant_id,
    (msg->>'conversationId')::uuid,
    p_path_id,
    msg->>'role',
    msg->>'content',
    (msg->>'sequenceInPath')::integer,
    COALESCE((msg->>'isBranchPoint')::boolean, false),
    COALESCE((msg->'branchedToPaths')::uuid[], '{}'),
    COALESCE(msg->'metadata', '{}'::jsonb),
    COALESCE((msg->>'createdAt')::timestamptz, now())
  FROM jsonb_array_elements(p_messages) AS msg;

  -- Transaction commits automatically on function exit
END;
$$ LANGUAGE plpgsql;
```

**Usage**:
```typescript
await this.supabase.rpc('replace_path_messages', {
  p_path_id: pathId,
  p_tenant_id: tenantId,
  p_messages: JSON.stringify(compactedMessages),
});
```

---

## 6. Merge Compaction

### 6.1 What is Merge Compaction?

**Merge Compaction** is the process of reducing the size of messages from a **branch** before merging them into the **parent path**.

**Context**: When users create branches to explore different directions, they often want to merge results back to the main conversation. Without compaction, full merges can bloat the main conversation.

**Merge Modes**:
1. **Summary** - AI summarizes entire branch into 1 message (existing, works well)
2. **Selective** - User picks specific messages to merge (no compaction needed)
3. **Full** - Copy all messages (âš ï¸ **this is where compaction is critical**)

### 6.2 Full Merge Problem

**Current Behavior** (problematic):
```
Branch (85 messages, 42,000 tokens)
  â†“ FULL MERGE (no compaction)
Main Path (100 messages, 58,000 tokens)
  â†“ RESULT
Main Path (185 messages, 100,000 tokens) âš ï¸ BLOATED
```

**New Behavior** (with compaction):
```
Branch (85 messages, 42,000 tokens)
  â†“ COMPACTION (moderate strategy)
Branch Compacted (35 messages, 18,000 tokens)
  â†“ FULL MERGE
Main Path (100 messages, 58,000 tokens)
  â†“ RESULT
Main Path (135 messages, 76,000 tokens) âœ… MANAGED
```

### 6.3 Merge Compaction Flow

```mermaid
sequenceDiagram
    participant User
    participant UI
    participant API
    participant PathStore
    participant Compactor
    participant TokenCounter
    participant LLM

    User->>UI: Click "Merge Branch"
    UI->>UI: Show MergeDialog
    User->>UI: Select "Full Merge"
    UI->>API: POST /merge (full mode)
    API->>PathStore: mergePath(sourceId, targetId, 'full')
    PathStore->>PathStore: Load source messages
    PathStore->>PathStore: Check: should compact?
    alt Should Compact (>15 messages)
        PathStore->>TokenCounter: Count source tokens
        TokenCounter-->>PathStore: 42,000 tokens
        PathStore->>PathStore: Load config (strategy=moderate)
        PathStore->>Compactor: compact(source messages)
        Compactor->>LLM: Identify redundant exchanges
        LLM-->>Compactor: Redundancy analysis
        Compactor->>LLM: Summarize redundant content
        LLM-->>Compactor: Summary
        Compactor-->>PathStore: CompactionResult (35 msgs, 18k tokens)
        PathStore->>PathStore: Use compacted messages
    end
    PathStore->>PathStore: Append to target path
    PathStore->>PathStore: Mark source as merged
    PathStore-->>API: MergeResult
    API-->>UI: Success
    UI-->>User: "Branch merged (35 messages)"
```

### 6.4 Merge Compaction API

**Updated MergeRequest**:
```typescript
interface MergeRequest {
  sourcePathId: string;
  targetPathId: string;
  mergeMode: 'summary' | 'full' | 'selective';
  selectedMessageIds?: string[];      // For selective mode
  summaryContent?: string;            // For summary mode

  // NEW: Compaction options
  applyCompaction?: boolean;          // Default: true for full merge
  compressionStrategy?: MergeCompressionStrategy;  // Override config
  compactionConfig?: Partial<CompactionConfig>;    // Additional config
}
```

**Implementation**:
```typescript
async mergePath(input: MergeRequest): Promise<MergeResult> {
  // Load config
  const config = await this.configStore.getConfig({
    tenantId: input.tenantId,
    userId: input.userId,
  });

  // Get source messages
  const sourceMessages = await this.resolvePathMessages({
    tenantId: input.tenantId,
    pathId: input.sourcePathId,
  });

  let messagesToMerge: ConversationMessage[];

  switch (input.mergeMode) {
    case 'summary':
      // Use AI summarization (existing implementation)
      messagesToMerge = await this.summarizeBranch(sourceMessages, input);
      break;

    case 'selective':
      // User-selected messages (no compaction)
      messagesToMerge = sourceMessages.filter(m =>
        input.selectedMessageIds?.includes(m.id)
      );
      break;

    case 'full':
      // Full merge with optional compaction
      messagesToMerge = sourceMessages;

      // Determine if compaction should apply
      const shouldCompact = input.applyCompaction ??
        (messagesToMerge.length > 15);  // Default threshold

      if (shouldCompact) {
        // Apply merge compaction
        const compacted = await this.applyMergeCompaction({
          messages: messagesToMerge,
          tenantId: input.tenantId,
          strategy: input.compressionStrategy ?? config.mergeCompressionStrategy,
          config: {
            preservePinned: config.mergePreservePinned,
            maxMessages: config.mergeMaxMessages,
            maxTokens: config.mergeMaxTokens,
            ...input.compactionConfig,
          },
        });

        messagesToMerge = compacted;
      }
      break;
  }

  // Append messages to target path
  const mergedMessageIds = await this.appendMessages({
    tenantId: input.tenantId,
    pathId: input.targetPathId,
    messages: messagesToMerge,
  });

  // Mark source path as merged
  await this.markPathAsMerged({
    sourcePathId: input.sourcePathId,
    targetPathId: input.targetPathId,
  });

  return {
    success: true,
    mergedMessageIds,
    targetPath: await this.getPath({ tenantId: input.tenantId, pathId: input.targetPathId }),
  };
}
```

### 6.5 Compaction Preview (UX Enhancement)

Before merging, show user a preview of compaction:

```typescript
async previewMergeCompaction(input: {
  sourcePathId: string;
  strategy: MergeCompressionStrategy;
}): Promise<MergeCompactionPreview> {
  const messages = await this.resolvePathMessages({
    pathId: input.sourcePathId,
  });

  const tokenCounter = createTokenCounterForModel('gpt-4');
  const currentTokens = (await tokenCounter.estimateContextTokens(messages)).tokens;

  // Run compaction
  const compactor = createMergeCompactor(input.strategy, {});
  const result = await compactor.compact({
    messages,
    currentTokens,
    // ... other context
  });

  return {
    currentMessageCount: messages.length,
    compactedMessageCount: result.messages.length,
    messagesRemoved: result.messagesRemoved,
    messagesSummarized: result.messagesSummarized,
    currentTokens: result.tokensBefore,
    compactedTokens: result.tokensAfter,
    reductionPercent: calculateReduction(result.tokensBefore, result.tokensAfter),
    strategy: input.strategy,
  };
}
```

**UI Usage**:
```tsx
// In MergeDialog
const preview = await previewMergeCompaction({
  sourcePathId: branch.id,
  strategy: 'moderate',
});

return (
  <div className="compaction-preview">
    <h4>Compaction Preview (Moderate Strategy)</h4>
    <div>Messages: {preview.currentMessageCount} â†’ {preview.compactedMessageCount}</div>
    <div>Tokens: {preview.currentTokens} â†’ {preview.compactedTokens}</div>
    <div>Reduction: {preview.reductionPercent}%</div>
    <div>Removed: {preview.messagesRemoved} redundant messages</div>
    <div>Summarized: {preview.messagesSummarized} exchanges</div>
  </div>
);
```

---

## 7. Configuration System

### 7.1 Configuration Hierarchy

Configuration applies at three levels with inheritance:

```
Global Config (applies to all)
  â†“ overridden by
Tenant Config (applies to organization)
  â†“ overridden by
User Config (personal preferences)
```

**Example**:
```typescript
// Global default
{
  mergeCompressionStrategy: 'moderate',
  pathCompressionStrategy: 'sliding_window',
}

// Tenant "acme-corp" overrides
{
  mergeCompressionStrategy: 'aggressive',  // Override
  // pathCompressionStrategy: 'sliding_window'  // Inherited from global
}

// User "alice" in "acme-corp" overrides
{
  // mergeCompressionStrategy: 'aggressive'  // Inherited from tenant
  pathCompressionStrategy: 'hybrid',  // Override
}

// Effective config for Alice:
{
  mergeCompressionStrategy: 'aggressive',  // From tenant
  pathCompressionStrategy: 'hybrid',        // From user
}
```

### 7.2 Configuration Schema

```typescript
interface ConversationConfig {
  // Merge compression
  mergeCompressionStrategy: MergeCompressionStrategy;
  mergeMaxMessages?: number;          // Max messages to keep in merged path
  mergeMaxTokens?: number;            // Max tokens for merged content
  mergePreservePinned?: boolean;      // Always preserve pinned messages

  // Path compression
  pathCompressionStrategy: PathCompressionStrategy;
  pathMaxMessages?: number;           // Trigger compaction at this count
  pathMaxTokens?: number;             // Trigger compaction at this token count
  pathSlidingWindowSize?: number;     // Window size for sliding_window strategy
  pathCompressionThreshold?: number;  // Similarity threshold for semantic

  // Auto-compaction
  autoCompactEnabled?: boolean;       // Enable automatic path compaction
  compactionIntervalMinutes?: number; // How often to check for compaction

  // Metadata
  configLevel: 'global' | 'tenant' | 'user';
  configScope: string;                // 'global', tenant ID, or user ID
  updatedAt: Date;
  updatedBy?: string | null;
}

type MergeCompressionStrategy =
  | 'none'           // No compression
  | 'minimal'        // Deduplication only
  | 'moderate'       // Summarize redundant (default)
  | 'aggressive';    // Outcomes only

type PathCompressionStrategy =
  | 'none'           // No compression
  | 'sliding_window' // Keep recent N messages
  | 'semantic'       // LLM importance scoring
  | 'hybrid';        // Sliding window + semantic
```

### 7.3 Default Configuration

```typescript
export const DEFAULT_CONFIG: ConversationConfig = {
  // Merge compression
  mergeCompressionStrategy: 'moderate',
  mergeMaxMessages: 100,
  mergeMaxTokens: 50000,
  mergePreservePinned: true,

  // Path compression
  pathCompressionStrategy: 'sliding_window',
  pathMaxMessages: 200,
  pathMaxTokens: 100000,
  pathSlidingWindowSize: 50,
  pathCompressionThreshold: 0.5,

  // Auto-compaction
  autoCompactEnabled: true,
  compactionIntervalMinutes: 60,

  // Metadata
  configLevel: 'global',
  configScope: 'global',
  updatedAt: new Date(),
};
```

### 7.4 Configuration API

```typescript
interface ConversationConfigStore {
  /**
   * Get effective configuration for user/tenant
   * (resolves hierarchy: global â†’ tenant â†’ user)
   */
  getConfig(input: {
    tenantId: string;
    userId?: string | null;
  }): Promise<ConversationConfig>;

  /**
   * Set global configuration
   */
  setGlobalConfig(
    config: Partial<ConversationConfig>,
    updatedBy?: string
  ): Promise<void>;

  /**
   * Set tenant-level configuration
   */
  setTenantConfig(input: {
    tenantId: string;
    config: Partial<ConversationConfig>;
    updatedBy?: string;
  }): Promise<void>;

  /**
   * Set user-level configuration
   */
  setUserConfig(input: {
    tenantId: string;
    userId: string;
    config: Partial<ConversationConfig>;
    updatedBy?: string;
  }): Promise<void>;

  /**
   * Delete tenant config (fall back to global)
   */
  deleteTenantConfig(tenantId: string): Promise<void>;

  /**
   * Delete user config (fall back to tenant/global)
   */
  deleteUserConfig(input: {
    tenantId: string;
    userId: string;
  }): Promise<void>;
}
```

---

## 8. Data Model

### 8.1 Database Schema

**No new tables required**. Compaction uses existing tables:

```sql
-- Existing: conversation_paths table
CREATE TABLE copilot_internal.conversation_paths (
  id uuid PRIMARY KEY,
  conversation_id uuid NOT NULL,
  tenant_id uuid NOT NULL,
  parent_path_id uuid,
  branch_point_message_id uuid,
  name text,
  is_primary boolean DEFAULT false,
  is_active boolean DEFAULT true,
  merged_to_path_id uuid,
  merged_at timestamptz,
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now()
);

-- Existing: conversation_messages table
CREATE TABLE copilot_internal.conversation_messages (
  id uuid PRIMARY KEY,
  conversation_id uuid NOT NULL,
  path_id uuid NOT NULL,
  tenant_id uuid NOT NULL,
  role text NOT NULL,
  content text NOT NULL,
  sequence_in_path integer NOT NULL,
  is_branch_point boolean DEFAULT false,
  branched_to_paths uuid[],
  metadata jsonb,              -- Can include: { pinned: true, compactionSummary: {...} }
  deleted_at timestamptz,      -- Soft delete (used for compaction)
  created_at timestamptz DEFAULT now()
);

-- Existing: conversation_configs table
CREATE TABLE copilot_internal.conversation_configs (
  id uuid PRIMARY KEY,
  tenant_id uuid NOT NULL,
  user_id uuid,
  config_level text NOT NULL CHECK (config_level IN ('global', 'tenant', 'user')),

  merge_compression_strategy text NOT NULL,
  merge_max_messages integer,
  merge_max_tokens integer,
  merge_preserve_pinned boolean,

  path_compression_strategy text NOT NULL,
  path_max_messages integer,
  path_max_tokens integer,
  path_sliding_window_size integer,
  path_compression_threshold numeric(3,2),

  auto_compact_enabled boolean,
  compaction_interval_minutes integer,

  updated_at timestamptz DEFAULT now(),
  updated_by uuid,

  UNIQUE (config_level, tenant_id, user_id)
);
```

### 8.2 Message Metadata Extensions

Compaction adds metadata to messages:

```typescript
interface MessageMetadata {
  // Existing fields
  pinned?: boolean;

  // NEW: Compaction metadata
  compactionSummary?: {
    type: 'path_summary' | 'merge_summary';
    sourceMessageIds: string[];        // Messages that were summarized
    compactionStrategy: string;        // Strategy used
    originalMessageCount: number;      // How many messages were summarized
    tokensBeforeCompaction?: number;   // Original token count
    tokensAfterCompaction?: number;    // Compacted token count
    summarizedAt: string;              // ISO timestamp
  };
}
```

**Example**:
```json
{
  "type": "merge_summary",
  "sourceMessageIds": ["msg-1", "msg-2", "msg-3", "msg-4", "msg-5"],
  "compactionStrategy": "merge:moderate",
  "originalMessageCount": 5,
  "tokensBeforeCompaction": 2400,
  "tokensAfterCompaction": 450,
  "summarizedAt": "2025-12-30T12:34:56Z"
}
```

---

## 9. Integration Points

### 9.1 Integration with Path System

**Path Store** (`ConversationPathStore`):
- `compactPath()` - Compact a specific path
- `shouldCompactPath()` - Check if path needs compaction
- `mergePath()` - Merge with compaction applied

**Message Store**:
- Uses existing `conversation_messages` table
- Soft delete for compaction (set `deleted_at`)
- Metadata tracks compaction history

### 9.2 Integration with LLM Router

**LLM Router** provides:
- Semantic scoring for message importance
- Redundancy detection for merge compaction
- Outcome extraction for aggressive compression
- Summarization of compacted content

**Required Capabilities**:
```typescript
interface LlmRouter {
  complete(request: {
    system: string;
    user: string;
    temperature?: number;
    maxTokens?: number;
  }): Promise<string>;
}
```

### 9.3 Integration with Chat API

**Before LLM Call**:
```typescript
// In /api/chat handler
async function handleChatRequest(req: ChatRequest) {
  const pathId = req.pathId;

  // Check if path needs compaction
  const needsCompaction = await pathStore.shouldCompactPath({
    tenantId: req.tenantId,
    pathId,
  });

  if (needsCompaction) {
    // Compact before expensive LLM call
    await pathStore.compactPath({
      tenantId: req.tenantId,
      pathId,
    });
  }

  // Now make LLM call with compacted context
  const messages = await pathStore.resolvePathMessages({ pathId });
  const response = await llmRouter.chat({ messages });

  return response;
}
```

### 9.4 Integration with UI Components

**PathToolbar**:
```tsx
function PathToolbar({ pathId }) {
  const [tokenStats, setTokenStats] = useState(null);

  useEffect(() => {
    // Load token stats
    fetch(`/api/paths/${pathId}/token-stats`)
      .then(res => res.json())
      .then(setTokenStats);
  }, [pathId]);

  return (
    <div className="path-toolbar">
      {tokenStats && (
        <div className="token-indicator">
          <Gauge size={16} />
          <span>{formatTokens(tokenStats.current)} / {formatTokens(tokenStats.max)}</span>
          {tokenStats.needsCompaction && (
            <Button size="sm" onClick={handleCompact}>
              Compact ({tokenStats.estimatedSavings} tokens)
            </Button>
          )}
        </div>
      )}
    </div>
  );
}
```

**MergeDialog**:
```tsx
function MergeDialog({ branchId, onMerge }) {
  const [compressionStrategy, setCompressionStrategy] = useState('moderate');
  const [preview, setPreview] = useState(null);

  useEffect(() => {
    // Load compaction preview
    fetch(`/api/branches/${branchId}/merge-preview?strategy=${compressionStrategy}`)
      .then(res => res.json())
      .then(setPreview);
  }, [branchId, compressionStrategy]);

  return (
    <Dialog>
      <h3>Merge Branch</h3>

      <RadioGroup value={compressionStrategy} onChange={setCompressionStrategy}>
        <Radio value="none">None - Copy all ({preview?.currentTokens} tokens)</Radio>
        <Radio value="minimal">Minimal - Remove duplicates ({preview?.minimalTokens} tokens)</Radio>
        <Radio value="moderate">Moderate - Summarize redundant ({preview?.moderateTokens} tokens)</Radio>
        <Radio value="aggressive">Aggressive - Outcomes only ({preview?.aggressiveTokens} tokens)</Radio>
      </RadioGroup>

      {preview && (
        <div className="preview">
          <p>Estimated savings: {preview.tokensSaved} tokens ({preview.reductionPercent}%)</p>
          <p>Messages: {preview.currentCount} â†’ {preview.compactedCount}</p>
        </div>
      )}

      <Button onClick={() => onMerge({ strategy: compressionStrategy })}>
        Merge with Compaction
      </Button>
    </Dialog>
  );
}
```

---

## 10. Performance & Scalability

### 10.1 Performance Characteristics

| Operation | Time Complexity | Typical Duration | Optimization |
|-----------|----------------|------------------|--------------|
| **Token Counting (tiktoken)** | O(n) | 1-5ms per message | LRU cache (60-80% hit rate) |
| **Sliding Window** | O(n) | 10-50ms for 200 msgs | No LLM calls, fast |
| **Semantic Scoring** | O(n) + LLM | 2-5s for 100 msgs | Batch scoring, parallel calls |
| **Hybrid Compaction** | O(n) + LLM | 1-3s for 200 msgs | Score old messages only |
| **Merge Moderate** | O(n) + LLM | 3-7s for 50 msgs | Batch redundancy detection |
| **Merge Aggressive** | O(n) + LLM | 2-4s for 100 msgs | Single extraction call |

### 10.2 Scalability Bottlenecks

**1. LLM Call Latency**

**Problem**: Semantic strategies require LLM calls, adding 2-5s latency.

**Solutions**:
- **Batch scoring**: Score multiple messages in one LLM call
- **Parallel calls**: Use multiple concurrent LLM requests
- **Caching**: Cache importance scores by content hash
- **Smart sampling**: Score representative samples, interpolate others

**2. Token Counting Performance**

**Problem**: Counting tokens for 1000+ messages can be slow.

**Solutions**:
- **LRU cache**: Cache token counts per message content (60-80% hit rate)
- **Batch processing**: Count multiple messages in parallel
- **Incremental counting**: Only count new messages, cache previous totals

**3. Message Replacement Atomicity**

**Problem**: Replacing 100+ messages requires transaction.

**Solutions**:
- **PostgreSQL function**: Use `replace_path_messages()` RPC for atomic operation
- **Soft deletion**: Mark messages as deleted instead of hard delete
- **Background jobs**: Queue large compactions for async processing

### 10.3 Optimization Strategies

**Token Counting Optimization**:
```typescript
class OptimizedTokenCounter {
  private cache = new LRUCache<string, number>(1000);

  async estimateContextTokensBatch(
    messages: ConversationMessage[]
  ): Promise<TokenEstimate> {
    const BATCH_SIZE = 50;
    const batches = chunk(messages, BATCH_SIZE);

    // Process batches in parallel
    const results = await Promise.all(
      batches.map(batch => this.processBatch(batch))
    );

    const totalTokens = results.reduce((sum, r) => sum + r.tokens, 0);

    return { tokens: totalTokens, method: 'tiktoken', isExact: true };
  }

  private async processBatch(batch: ConversationMessage[]): Promise<TokenEstimate> {
    // Check cache for each message
    const uncached = batch.filter(m => !this.cache.has(m.content));

    if (uncached.length === 0) {
      // All cached!
      const tokens = batch.reduce((sum, m) => sum + this.cache.get(m.content)!, 0);
      return { tokens, method: 'cached', isExact: true };
    }

    // Count uncached messages
    const tokens = await this.countMessages(uncached);

    // Cache results
    uncached.forEach((m, idx) => this.cache.set(m.content, tokens[idx]));

    return { tokens: sum(tokens), method: 'tiktoken', isExact: true };
  }
}
```

**LLM Call Batching**:
```typescript
class SemanticCompactor {
  private async scoreMessagesBatch(
    messages: ConversationMessage[]
  ): Promise<number[]> {
    const BATCH_SIZE = 20;  // Score 20 messages per LLM call
    const batches = chunk(messages, BATCH_SIZE);

    // Process batches in parallel (max 3 concurrent)
    const allScores = await pMap(
      batches,
      batch => this.scoreSingleBatch(batch),
      { concurrency: 3 }
    );

    return allScores.flat();
  }
}
```

### 10.4 Resource Usage

**Token Counting**:
- Memory: ~5MB for LRU cache (1000 entries)
- CPU: Minimal (<1% for typical usage)
- Network: None (local computation)

**LLM-Based Strategies**:
- Memory: ~10MB for message buffers
- CPU: Minimal (I/O bound)
- Network: 2-10 API calls per compaction
- Cost: $0.01-0.05 per compaction (GPT-4)

**Database Operations**:
- Connections: Reuses existing connection pool
- Transactions: 1 per compaction
- Lock duration: <100ms (soft delete + insert)

---

## 11. Security & Privacy

### 11.1 Data Privacy

**Principle**: Compaction MUST NOT leak data across tenant boundaries.

**Enforcement**:
1. **Tenant Isolation**: All operations scoped to `tenantId`
2. **RLS Policies**: PostgreSQL RLS enforces tenant boundaries
3. **LLM Calls**: No tenant identifiers sent to LLM (only message content)

**Example**:
```typescript
// GOOD: Tenant scoped
await pathStore.compactPath({
  tenantId: 'acme-corp',
  pathId: 'path-123',
});

// BAD: Missing tenant check (RLS will reject)
await supabase
  .from('conversation_messages')
  .select('*')
  .eq('path_id', 'path-123');  // âŒ No tenant check!
```

### 11.2 Pinned Message Security

**Threat**: Compaction accidentally removes critical compliance messages.

**Mitigation**:
1. **Validation**: All compactors MUST call `validatePinnedPreservation()`
2. **Auditing**: Log all compaction operations with pinned message counts
3. **Rollback**: Keep soft-deleted messages for 30 days (recovery)

**Validation**:
```typescript
protected validatePinnedPreservation(
  context: CompactionContext,
  result: ConversationMessage[]
): void {
  if (!this.config.preservePinned) return;

  const resultIds = new Set(result.map(m => m.id));
  const missingPinned = Array.from(context.pinnedMessageIds).filter(
    id => !resultIds.has(id)
  );

  if (missingPinned.length > 0) {
    // CRITICAL ERROR - abort compaction
    throw new Error(
      `Compaction violated pinned message preservation: missing ${missingPinned.length} pinned messages. IDs: ${missingPinned.join(', ')}`
    );
  }
}
```

### 11.3 LLM Security

**Threat**: Sensitive data sent to LLM during semantic compaction.

**Mitigation**:
1. **PII Redaction**: Optionally redact PII before LLM calls (future enhancement)
2. **Self-Hosted LLM**: Support local LLM models for sensitive tenants
3. **Audit Logging**: Log all LLM calls with tenant/user info

**Example**:
```typescript
// Future: PII redaction
async function scoreMessages(messages: ConversationMessage[]): Promise<number[]> {
  const redacted = messages.map(m => ({
    ...m,
    content: redactPII(m.content),  // Remove emails, SSNs, etc.
  }));

  return await this.llmRouter.scoreImportance(redacted);
}
```

### 11.4 Authorization

**Access Control**:
- Users can only compact paths they have access to (enforced by RLS)
- Tenant admins can configure compaction strategies
- Global admins can set global defaults

**Configuration Access**:
```typescript
// Check: Can user set tenant config?
if (!isAdminForTenant(userId, tenantId)) {
  throw new Error('Unauthorized: tenant admin required');
}

await configStore.setTenantConfig({
  tenantId,
  config: { mergeCompressionStrategy: 'aggressive' },
});
```

---

## 12. Future Enhancements

### 12.1 Advanced Compaction Strategies

**1. Time-Aware Compaction**

Preserve recent messages from last N days, regardless of count:
```typescript
class TimeAwareCompactor extends MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    const { messages, pinnedMessageIds } = context;
    const retentionDays = this.config.retentionDays ?? 7;
    const cutoff = subDays(new Date(), retentionDays);

    const recent = messages.filter(m => m.createdAt >= cutoff);
    const old = messages.filter(m => m.createdAt < cutoff);

    // Keep all recent, summarize old
    const summarized = await this.summarize(old);
    return this.combine([...pinnedMessages, ...recent, summarized]);
  }
}
```

**2. Topic-Based Compaction**

Group messages by topic, compress within topics:
```typescript
class TopicBasedCompactor extends MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    // Use LLM to identify topics
    const topics = await this.identifyTopics(context.messages);

    // Compress each topic independently
    const compacted = await Promise.all(
      topics.map(topic => this.compressTopic(topic))
    );

    return this.merge(compacted);
  }
}
```

**3. User-Guided Compaction**

Let users mark messages as "important" (beyond just pinning):
```typescript
interface MessageImportance {
  messageId: string;
  importance: 'critical' | 'high' | 'medium' | 'low';
  userSet: boolean;  // User explicitly set this
}

// Use user-set importance to guide compaction
class UserGuidedCompactor extends MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    const userImportance = await this.loadUserImportance(context.messages);

    // Keep critical + high importance
    const important = context.messages.filter(m =>
      userImportance[m.id] === 'critical' ||
      userImportance[m.id] === 'high'
    );

    // Apply semantic to medium/low
    const rest = context.messages.filter(m =>
      userImportance[m.id] === 'medium' ||
      userImportance[m.id] === 'low'
    );

    const compactedRest = await this.semanticCompact(rest);

    return this.combine([...important, ...compactedRest]);
  }
}
```

### 12.2 Compaction Analytics

**Dashboard Metrics**:
- Total tokens saved per day/week/month
- Average reduction percentage per strategy
- Most effective strategy per use case
- Cost savings from reduced LLM token usage

**Example Metrics**:
```typescript
interface CompactionMetrics {
  totalCompactions: number;
  totalTokensSaved: number;
  totalMessagesSaved: number;
  averageReduction: number;  // Percentage
  costSavingsUSD: number;
  byStrategy: {
    [strategy: string]: {
      count: number;
      avgReduction: number;
      avgDurationMs: number;
    };
  };
}
```

### 12.3 Smart Auto-Compaction

**Adaptive Thresholds**:

Learn optimal compaction thresholds per user/tenant:
```typescript
class AdaptiveCompactionScheduler {
  async determineThreshold(pathId: string): Promise<number> {
    // Analyze user behavior
    const stats = await this.getPathStats(pathId);

    // If user frequently encounters token limits, lower threshold
    if (stats.tokenLimitErrors > 3) {
      return 150;  // Compact earlier
    }

    // If path is rarely accessed, use higher threshold
    if (stats.messagesPerDay < 5) {
      return 300;  // Delay compaction
    }

    return 200;  // Default
  }
}
```

**Predictive Compaction**:

Compact before user hits limits:
```typescript
async function predictiveCompact(pathId: string): Promise<void> {
  const messages = await pathStore.resolvePathMessages({ pathId });
  const growthRate = calculateGrowthRate(messages);

  // Project when path will hit token limit
  const currentTokens = await countTokens(messages);
  const projectedLimit = predictLimit(currentTokens, growthRate);

  if (projectedLimit < 5000) {  // Less than 5k tokens until limit
    // Compact proactively
    await pathStore.compactPath({ pathId });
  }
}
```

### 12.4 Multi-Modal Compaction

**Support Non-Text Content**:

Handle images, code blocks, tables differently:
```typescript
class MultiModalCompactor extends MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    const { text, code, images, tables } = this.partition(context.messages);

    // Different strategies for different content
    const compactedText = await this.compactText(text);
    const compactedCode = await this.compactCode(code);  // Keep syntax, remove comments
    const compactedImages = this.compactImages(images);  // Thumbnails
    const compactedTables = this.compactTables(tables);  // Summarize large tables

    return this.combine([
      compactedText,
      compactedCode,
      compactedImages,
      compactedTables,
    ]);
  }
}
```

### 12.5 Collaborative Compaction

**Multi-User Scenarios**:

Allow multiple users to vote on important messages:
```typescript
interface CollaborativeImportance {
  messageId: string;
  votes: {
    userId: string;
    importance: number;  // 0-1
  }[];
  consensusScore: number;  // Average or weighted average
}

// Use consensus to guide compaction
class CollaborativeCompactor extends MessageCompactor {
  async compact(context: CompactionContext): Promise<CompactionResult> {
    const consensus = await this.loadConsensus(context.messages);

    // Keep messages with high consensus score
    const important = context.messages.filter(m =>
      consensus[m.id]?.consensusScore >= 0.7
    );

    // Compact messages with low consensus
    const unimportant = context.messages.filter(m =>
      consensus[m.id]?.consensusScore < 0.7
    );

    const compacted = await this.summarize(unimportant);

    return this.combine([...important, compacted]);
  }
}
```

---

## Conclusion

The **Conversation Compaction Architecture** provides a comprehensive solution to managing conversation context size in the Regulatory Intelligence Copilot.

### Key Achievements

1. **âœ… Token Counting Infrastructure** - Accurate, cached, tiktoken-based measurement
2. **âœ… 8 Compaction Strategies** - Flexible algorithms for different use cases
3. **âœ… Path Compaction** - Automatic compression of growing conversations
4. **âœ… Merge Compaction** - Intelligent compression during branch merges
5. **âœ… Pinned Message Preservation** - Always respect user-marked content
6. **âœ… Hierarchical Configuration** - Global, tenant, and user-level settings
7. **âœ… Performance Optimized** - Caching, batching, parallel processing
8. **âœ… Security First** - Tenant isolation, pinned validation, audit logging

### Impact Estimate

**Quantitative**:
- 50%+ reduction in context tokens for large branches
- 30%+ reduction in LLM API costs
- 0 token limit exceeded errors
- <5s compaction time for 500 messages

**Qualitative**:
- Improved LLM response quality (less context bloat)
- Better user control over conversation size
- Clear visibility into token usage
- Smoother merge experience

### Implementation Status

**Current**: ğŸ”µ Planned (Architecture Complete, Ready for Implementation)

**Next Steps**:
1. Review and approve this architecture
2. Begin implementation following the [Implementation Plan](../development/implementation-plans/COMPACTION_STRATEGIES_IMPLEMENTATION_PLAN.md)
3. Phase 1: Token counting infrastructure
4. Phase 2: Basic strategies (none, sliding_window, minimal)
5. Phase 3: LLM-based strategies (semantic, moderate, aggressive, hybrid)
6. Phase 4: Integration and testing
7. Phase 5: UI and rollout

---

**Document Version**: 1.0
**Last Updated**: 2025-12-30
**Author**: Claude Code
**Status**: ğŸŸ¢ Approved for Implementation

**Related Documents**:
- Implementation Plan: `docs/development/implementation-plans/COMPACTION_STRATEGIES_IMPLEMENTATION_PLAN.md`
- Message Pinning: `MESSAGE_PINNING.md`
- Branching & Merging: `conversation-branching-and-merging.md`
- Configuration: `packages/reg-intel-conversations/src/conversationConfig.ts`
