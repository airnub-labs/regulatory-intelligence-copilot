name: regulatory-intelligence-copilot

services:
  # OpenTelemetry Collector for observability (logs, traces, metrics)
  # Development config: ./otel-collector-config.yaml
  # Production config: ./otel-collector-config.production.yaml
  #
  # To use production config:
  #   docker compose -f docker-compose.yml -f docker-compose.production.yml up
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: reg-copilot-otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
      - otel_logs:/var/log/otel
      - otel_file_storage:/var/lib/otel/file_storage  # For production queue persistence
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver (used by Next.js and scripts)
      - "8888:8888"   # Prometheus metrics (collector self-monitoring)
      - "8889:8889"   # Prometheus exporter (application metrics)
      - "13133:13133" # Health check
      - "55679:55679" # zPages (debugging)
    environment:
      - OTEL_LOG_LEVEL=info
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - observability

  # Jaeger for trace visualization
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: reg-copilot-jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    restart: unless-stopped
    networks:
      - observability

  # Prometheus for metrics storage and querying
  prometheus:
    image: prom/prometheus:latest
    container_name: reg-copilot-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    restart: unless-stopped
    depends_on:
      otel-collector:
        condition: service_healthy
    networks:
      - observability

  # Loki for log aggregation
  loki:
    image: grafana/loki:latest
    container_name: reg-copilot-loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    ports:
      - "3100:3100"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - observability

  # Grafana for unified observability dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: reg-copilot-grafana
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    ports:
      - "3200:3000"  # Map to 3200 to avoid conflict with Next.js
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_AUTH_ANONYMOUS_ENABLED=false
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
      loki:
        condition: service_healthy
      jaeger:
        condition: service_started
    networks:
      - observability

  # Redis for distributed rate limiting and caching
  redis:
    image: redis:7-alpine
    container_name: reg-copilot-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --requirepass "${REDIS_PASSWORD:-devpassword}"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    networks:
      - observability

  # Memgraph database with Lab UI and MAGE, schema info enabled (required for MCP)
  memgraph:
    image: memgraph/memgraph-platform:latest
    ports:
      - "7687:7687"
      - "7444:3000"  # Map Lab's internal port 3000 to host port 7444
    volumes:
      - memgraph_data:/var/lib/memgraph
      - memgraph_log:/var/log/memgraph
    environment:
      # Pass flags to Memgraph database via MEMGRAPH env var
      - MEMGRAPH=--telemetry-enabled=false
    healthcheck:
      test: ["CMD-SHELL", "echo 'RETURN 1;' | mgconsole || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  # Memgraph MCP Server - built from official ai-toolkit repo
  # https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph
  memgraph-mcp:
    build:
      context: ./memgraph-mcp
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    environment:
      - MEMGRAPH_URL=bolt://memgraph:7687
      - MEMGRAPH_USER=memgraph
      - MEMGRAPH_USERNAME=memgraph
      - MEMGRAPH_PASSWORD=memgraph
      - MCP_TRANSPORT=streamable-http
      - MCP_READ_ONLY=false
    depends_on:
      memgraph:
        condition: service_healthy

volumes:
  memgraph_data:
  memgraph_log:
  otel_logs:
  prometheus_data:
  loki_data:
  grafana_data:
  redis_data:

networks:
  observability:
    driver: bridge
